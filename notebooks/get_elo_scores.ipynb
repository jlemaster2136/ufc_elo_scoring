{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fights data from https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/refs/heads/main/ufc_fight_results.csv.\n",
      "Cleaning fights data.\n",
      "Loading events data from https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/refs/heads/main/ufc_event_details.csv.\n",
      "Merging fights data with events data.\n",
      "Adding WINNER column.\n",
      "Mapping fight finishing methods to numerical values.\n",
      "\n",
      "Evaluating k=1\n",
      "Calculating Elo ratings with k_base=1.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=1: 0.59\n",
      "\n",
      "Evaluating k=2\n",
      "Calculating Elo ratings with k_base=2.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=2: 0.59\n",
      "\n",
      "Evaluating k=3\n",
      "Calculating Elo ratings with k_base=3.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=3: 0.59\n",
      "\n",
      "Evaluating k=4\n",
      "Calculating Elo ratings with k_base=4.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=4: 0.58\n",
      "\n",
      "Evaluating k=5\n",
      "Calculating Elo ratings with k_base=5.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=5: 0.58\n",
      "\n",
      "Evaluating k=6\n",
      "Calculating Elo ratings with k_base=6.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=6: 0.58\n",
      "\n",
      "Evaluating k=7\n",
      "Calculating Elo ratings with k_base=7.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=7: 0.58\n",
      "\n",
      "Evaluating k=8\n",
      "Calculating Elo ratings with k_base=8.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=8: 0.58\n",
      "\n",
      "Evaluating k=9\n",
      "Calculating Elo ratings with k_base=9.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=9: 0.59\n",
      "\n",
      "Evaluating k=10\n",
      "Calculating Elo ratings with k_base=10.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=10: 0.60\n",
      "\n",
      "Evaluating k=15\n",
      "Calculating Elo ratings with k_base=15.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=15: 0.59\n",
      "\n",
      "Evaluating k=50\n",
      "Calculating Elo ratings with k_base=50.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=50: 0.59\n",
      "\n",
      "Evaluating k=70\n",
      "Calculating Elo ratings with k_base=70.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=70: 0.60\n",
      "\n",
      "Evaluating k=100\n",
      "Calculating Elo ratings with k_base=100.\n",
      "Processing fight history into DataFrame.\n",
      "Calculating total fight time.\n",
      "Model Accuracy for k=100: 0.58\n",
      "\n",
      "Best k value: 10 with accuracy: 0.60\n",
      "Saving fight history to /workspaces/codespaces-jupyter/data/ufc_fight_results_with_elo.csv.\n",
      "Fight history saved successfully.\n",
      "\n",
      "Results saved to: /workspaces/codespaces-jupyter/data/ufc_fight_results_with_elo.csv\n",
      "\n",
      "Sample of processed fight history with the best k value:\n",
      "            fighter1           fighter2             winner  rating1_before  rating2_before  rating1_after  rating2_after                           event       date        time_format  round timestamp  fighter1_outcome  fighter2_outcome  total_time_minutes  total_time_seconds\n",
      "0    Islam Makhachev     Renato Moicano    Islam Makhachev     1993.062629     1752.208461    2009.060406    1736.210684  UFC 311: Makhachev vs. Moicano 2025-01-18  5 Rnd (5-5-5-5-5)      1      4:05                 1                 0            4.083333               245.0\n",
      "1      Bogdan Guskov      Billy Elekana      Bogdan Guskov     1571.824869     1500.000000    1603.671544    1468.153324  UFC 311: Makhachev vs. Moicano 2025-01-18      3 Rnd (5-5-5)      2      3:33                 1                 0            8.550000               513.0\n",
      "2  Merab Dvalishvili  Umar Nurmagomedov  Merab Dvalishvili     1924.054332     1760.655840    1949.324505    1735.385667  UFC 311: Makhachev vs. Moicano 2025-01-18  5 Rnd (5-5-5-5-5)      5      5:00                 1                 0           25.000000              1500.0\n",
      "3     Jiri Prochazka       Jamahal Hill     Jiri Prochazka     1672.471118     1710.281195    1722.349053    1660.403260  UFC 311: Makhachev vs. Moicano 2025-01-18      3 Rnd (5-5-5)      3      3:01                 1                 0           13.016667               781.0\n",
      "4    Jailton Almeida     Serghei Spivac    Jailton Almeida     1733.576319     1691.062888    1773.097280    1651.541927  UFC 311: Makhachev vs. Moicano 2025-01-18      3 Rnd (5-5-5)      1      4:53                 1                 0            4.883333               293.0\n",
      "\n",
      "Top 50 fighters by final Elo rating:\n",
      "                    fighter  final_rating\n",
      "595               Jon Jones   2097.364569\n",
      "308       Georges St-Pierre   2017.159034\n",
      "1437        Islam Makhachev   2009.060406\n",
      "1551         Belal Muhammad   1987.530095\n",
      "925     Khabib Nurmagomedov   1966.832075\n",
      "1456           Kamaru Usman   1951.547932\n",
      "1745      Merab Dvalishvili   1949.324505\n",
      "1507        Francis Ngannou   1948.065591\n",
      "932            Max Holloway   1939.888169\n",
      "1356           Leon Edwards   1935.316884\n",
      "749        Charles Oliveira   1924.682782\n",
      "1611  Alexander Volkanovski   1895.851437\n",
      "880            Stipe Miocic   1879.004468\n",
      "2214           Alex Pereira   1874.571889\n",
      "1016       Robert Whittaker   1873.069414\n",
      "2100      Dricus Du Plessis   1871.552817\n",
      "1771       Magomed Ankalaev   1869.808020\n",
      "1117           Amanda Nunes   1866.297333\n",
      "803      Demetrious Johnson   1865.048624\n",
      "1950             Ciryl Gane   1856.467021\n",
      "1907          Movsar Evloev   1852.200471\n",
      "1078         Daniel Cormier   1845.925545\n",
      "1509   Valentina Shevchenko   1844.909499\n",
      "1788               Petr Yan   1843.498503\n",
      "2101           Ilia Topuria   1839.161281\n",
      "2060           Tom Aspinall   1837.846979\n",
      "791          Dustin Poirier   1835.670149\n",
      "1055         Gegard Mousasi   1824.865196\n",
      "1205      Aljamain Sterling   1821.350187\n",
      "1221        Sean Strickland   1820.427651\n",
      "1812            Zhang Weili   1814.644806\n",
      "1330        Colby Covington   1812.088607\n",
      "1729          Sean O'Malley   1811.231419\n",
      "1604       Alexander Volkov   1810.744849\n",
      "2111      Shavkat Rakhmonov   1808.596385\n",
      "1903        Arman Tsarukyan   1807.229904\n",
      "502         Anthony Johnson   1807.200087\n",
      "2138           Manon Fiorot   1805.933659\n",
      "617              Ryan Bader   1805.893659\n",
      "1655    Deiveson Figueiredo   1804.009444\n",
      "1759        Israel Adesanya   1802.248129\n",
      "1668         Justin Gaethje   1802.224637\n",
      "897            TJ Dillashaw   1801.422148\n",
      "1891           Grant Dawson   1800.198034\n",
      "1350         Jan Blachowicz   1797.412811\n",
      "1398           Henry Cejudo   1797.086833\n",
      "831               Jose Aldo   1795.324411\n",
      "1633      Alexandre Pantoja   1792.762340\n",
      "567          Cain Velasquez   1790.379046\n",
      "1989             Sean Brady   1789.570807\n",
      "Final ratings saved to /workspaces/codespaces-jupyter/data/ufc_all_time_elo_rankings.csv.\n",
      "\n",
      "Top fighters rankings saved to: /workspaces/codespaces-jupyter/data/ufc_all_time_elo_rankings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def load_fights_data(filepath):\n",
    "    # Load and clean fights data\n",
    "    print(f\"Loading fights data from {filepath}.\")\n",
    "    fights = pd.read_csv(\n",
    "        filepath,\n",
    "        usecols=['EVENT', 'BOUT', 'OUTCOME', 'METHOD', 'TIME FORMAT', 'WEIGHTCLASS', 'ROUND', 'TIME']\n",
    "    )\n",
    "\n",
    "    print(\"Cleaning fights data.\")\n",
    "    fights['EVENT'] = fights['EVENT'].str.strip()\n",
    "    fights['BOUT'] = fights['BOUT'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    fights[['FIGHTER1', 'FIGHTER2']] = fights['BOUT'].str.split(' vs. ', expand=True)\n",
    "\n",
    "    return fights\n",
    "\n",
    "\n",
    "def load_events_data(filepath):\n",
    "    # Load and clean events data\n",
    "    print(f\"Loading events data from {filepath}.\")\n",
    "    events = pd.read_csv(\n",
    "        filepath,\n",
    "        usecols=['EVENT', 'DATE']\n",
    "    )\n",
    "    events['DATE'] = pd.to_datetime(events['DATE'])\n",
    "    return events\n",
    "\n",
    "\n",
    "def merge_fights_events(fights, events):\n",
    "    # Merge fights data with events data\n",
    "    print(\"Merging fights data with events data.\")\n",
    "    merged_data = pd.merge(fights, events, on='EVENT', how='left')\n",
    "    merged_data = merged_data.sort_values(by=\"DATE\", ascending=True).reset_index(drop=True)\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def determine_winner(row):\n",
    "    # Determine the winner of a fight based on the outcome value\n",
    "    outcome_map = {\n",
    "        \"W/L\": row[\"FIGHTER1\"],\n",
    "        \"L/W\": row[\"FIGHTER2\"]\n",
    "    }\n",
    "    return outcome_map.get(row[\"OUTCOME\"], None)\n",
    "\n",
    "\n",
    "def add_winner_column(data):\n",
    "    # Add a WINNER column\n",
    "    print(\"Adding WINNER column.\")\n",
    "    data[\"WINNER\"] = data.apply(determine_winner, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def map_finishing_methods(data):\n",
    "    # Map fight finishing methods to multiplier values\n",
    "    print(\"Mapping fight finishing methods to numerical values.\")\n",
    "    # These values were determined via general knowledge of mma and what is considered a \"good\" way of winning\n",
    "    fight_finishing_methods = {\n",
    "        \"Submission \": 8,\n",
    "        'SUB': 8,\n",
    "        \"KO/TKO \": 9,\n",
    "        \"KO/TKO\": 9,\n",
    "        \"TKO - Doctor's Stoppage \": 5,\n",
    "        \"Other \": 3,\n",
    "        \"Decision - Unanimous \": 9,\n",
    "        'U-DEC': 9,\n",
    "        \"Decision - Split \": 4,\n",
    "        'S-DEC': 4,\n",
    "        \"Overturned \": 1,\n",
    "        \"Decision - Majority \": 5,\n",
    "        \"DQ \": 3,\n",
    "        \"Could Not Continue \": 2,\n",
    "    }\n",
    "    data['METHOD'] = data['METHOD'].map(fight_finishing_methods)\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_elo_ratings(data, k_base, initial_elo=1500):\n",
    "    # Calculate Elo ratings\n",
    "    print(f\"Calculating Elo ratings with k_base={k_base}.\")\n",
    "    ratings = {}\n",
    "    history = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        fighter1 = row['FIGHTER1']\n",
    "        fighter2 = row['FIGHTER2']\n",
    "        winner = row['WINNER']\n",
    "        method = row['METHOD']\n",
    "        if pd.isnull(method):\n",
    "            # If METHOD is NaN, skip this fight as it affects k_factor\n",
    "            print(f\"Skipping fight between {fighter1} and {fighter2} due to missing METHOD.\")\n",
    "            continue\n",
    "        k_factor = method * k_base\n",
    "\n",
    "        # Assign initial elo score for first instance\n",
    "        rating1 = ratings.get(fighter1, initial_elo)\n",
    "        rating2 = ratings.get(fighter2, initial_elo)\n",
    "\n",
    "        # Calculate expected scores\n",
    "        exp_score1 = 1 / (1 + 10 ** ((rating2 - rating1) / 400))\n",
    "        exp_score2 = 1 - exp_score1\n",
    "\n",
    "        # Calculate actual scores\n",
    "        actual1 = 1 if winner == fighter1 else 0\n",
    "        actual2 = 1 - actual1\n",
    "\n",
    "        # Update ratings\n",
    "        new_rating1 = rating1 + k_factor * (actual1 - exp_score1)\n",
    "        new_rating2 = rating2 + k_factor * (actual2 - exp_score2)\n",
    "\n",
    "        ratings[fighter1] = new_rating1\n",
    "        ratings[fighter2] = new_rating2\n",
    "\n",
    "        # Record fight history\n",
    "        history.append({\n",
    "            'fighter1': fighter1,\n",
    "            'fighter2': fighter2,\n",
    "            'winner': winner,\n",
    "            'rating1_before': rating1,\n",
    "            'rating2_before': rating2,\n",
    "            'rating1_after': new_rating1,\n",
    "            'rating2_after': new_rating2,\n",
    "            'event': row['EVENT'],\n",
    "            'date': row['DATE'],\n",
    "            'time_format': row['TIME FORMAT'],\n",
    "            'round': row['ROUND'],\n",
    "            'timestamp': row['TIME']\n",
    "        })\n",
    "\n",
    "    return ratings, history\n",
    "\n",
    "\n",
    "def win_loss_numeric_f1(row):\n",
    "    # Convert fight outcome to numeric for fighter1\n",
    "    return 1 if row[\"fighter1\"] == row['winner'] else 0\n",
    "\n",
    "\n",
    "def win_loss_numeric_f2(row):\n",
    "    # Convert fight outcome to numeric for fighter2\n",
    "    return 1 if row[\"fighter2\"] == row['winner'] else 0\n",
    "\n",
    "\n",
    "def calculate_total_time(row):\n",
    "    # Calculate total fight time in minutes and seconds.\n",
    "    try:\n",
    "        minutes, seconds = map(int, row['timestamp'].split(':'))\n",
    "        full_rounds_time = (row['round'] - 1) * 5 * 60  # Each round is 5 minutes\n",
    "        last_round_time = minutes * 60 + seconds\n",
    "        total_time_seconds = full_rounds_time + last_round_time\n",
    "        total_time_minutes = total_time_seconds / 60\n",
    "        return total_time_minutes, total_time_seconds\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating total time for fight between {row['fighter1']} and {row['fighter2']} on {row['date']}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_history(history):\n",
    "    # Convert fight history to DataFrame and calculate additional metrics\n",
    "    print(\"Processing fight history into DataFrame.\")\n",
    "    history_df = pd.DataFrame(history)\n",
    "\n",
    "    # Sort by date\n",
    "    history_df = history_df.sort_values(by='date', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Generate outcome columns\n",
    "    history_df['fighter1_outcome'] = history_df.apply(win_loss_numeric_f1, axis=1)\n",
    "    history_df['fighter2_outcome'] = history_df.apply(win_loss_numeric_f2, axis=1)\n",
    "\n",
    "    # Calculate total fight time\n",
    "    print(\"Calculating total fight time.\")\n",
    "    history_df[['total_time_minutes', 'total_time_seconds']] = history_df.apply(\n",
    "        lambda row: pd.Series(calculate_total_time(row)), axis=1\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    history_df = history_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    return history_df\n",
    "\n",
    "\n",
    "def save_history(history_df, filepath):\n",
    "    # Save the fight history DataFrame to a CSV file\n",
    "    print(f\"Saving fight history to {filepath}.\")\n",
    "    history_upper = history_df.copy()\n",
    "    history_upper.columns = [col.upper() for col in history_upper.columns]\n",
    "    history_upper.to_csv(filepath, index=False)\n",
    "    print(\"Fight history saved successfully.\")\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(history_df):\n",
    "    # Train a Random Forest Classifier and evaluate its performance to find best k value\n",
    "    print(\"Training and evaluating the Random Forest model.\")\n",
    "\n",
    "    # Generate outcome columns\n",
    "    history_df['fighter1_outcome'] = history_df['fighter1_outcome'].astype(int)\n",
    "    history_df['fighter2_outcome'] = history_df['fighter2_outcome'].astype(int)\n",
    "\n",
    "    # Features and target\n",
    "    X = history_df[['rating1_before', 'rating2_before']]\n",
    "    y = history_df['fighter1_outcome']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize and train the classifier\n",
    "    rf_classifier = RandomForestClassifier(\n",
    "        n_estimators=100, random_state=42, class_weight='balanced'\n",
    "    )\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    print(\"Random Forest model trained.\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "\n",
    "def find_best_k(data, k_values, initial_elo=1500):\n",
    "    # Iterate through different k values to find the one with the highest model accuracy\n",
    "    best_k = None\n",
    "    best_accuracy = 0\n",
    "    best_history_df = None\n",
    "    best_ratings = None\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"\\nEvaluating k={k}\")\n",
    "        ratings, history = calculate_elo_ratings(data, k, initial_elo)\n",
    "        history_df = process_history(history)\n",
    "\n",
    "        # Check if history_df is empty to avoid training on empty data\n",
    "        if history_df.empty:\n",
    "            print(f\"No valid fights to process for k={k}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Train and evaluate the model\n",
    "        X = history_df[['rating1_before', 'rating2_before']]\n",
    "        y = history_df['fighter1_outcome']\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Initialize and train the classifier\n",
    "        rf_classifier = RandomForestClassifier(\n",
    "            n_estimators=100, random_state=42, class_weight='balanced'\n",
    "        )\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "        # Evaluation\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model Accuracy for k={k}: {accuracy:.2f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "            best_history_df = history_df\n",
    "            best_ratings = ratings\n",
    "\n",
    "    print(f\"\\nBest k value: {best_k} with accuracy: {best_accuracy:.2f}\")\n",
    "    return best_k, best_accuracy, best_history_df, best_ratings\n",
    "\n",
    "\n",
    "def display_top_fighters(history_df, ratings, top_n=40, save_csv=False, csv_path=''):\n",
    "    # Create DataFrame from top ratings\n",
    "    final_ratings_df = pd.DataFrame.from_dict(ratings, orient='index', \n",
    "                                            columns=['final_rating']).reset_index()\n",
    "    final_ratings_df.columns = ['fighter', 'final_rating']\n",
    "    \n",
    "    # Sort by final rating\n",
    "    final_ratings_df = final_ratings_df.sort_values(\n",
    "        by='final_rating', ascending=False).head(top_n)\n",
    "    \n",
    "    # Print top fighters\n",
    "    print(f\"\\nTop {top_n} fighters by final Elo rating:\")\n",
    "    print(final_ratings_df.to_string())\n",
    "\n",
    "    # Save to CSV\n",
    "    if save_csv and csv_path:\n",
    "        final_ratings_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Final ratings saved to {csv_path}.\")\n",
    "\n",
    "\n",
    "\n",
    "# Generate scores workflow\n",
    "\n",
    "# File paths\n",
    "fights_csv = r'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/refs/heads/main/ufc_fight_results.csv'\n",
    "events_csv = r'https://raw.githubusercontent.com/Greco1899/scrape_ufc_stats/refs/heads/main/ufc_event_details.csv'\n",
    "output_csv = r'/workspaces/codespaces-jupyter/data/ufc_fight_results_with_elo.csv'\n",
    "rankings_csv = r'/workspaces/codespaces-jupyter/data/ufc_all_time_elo_rankings.csv'\n",
    "\n",
    "# Load and preprocess data\n",
    "fights = load_fights_data(fights_csv)\n",
    "events = load_events_data(events_csv)\n",
    "data = merge_fights_events(fights, events)\n",
    "data = add_winner_column(data)\n",
    "data = map_finishing_methods(data)\n",
    "\n",
    "# List of k values to evaluate\n",
    "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50, 70, 100]\n",
    "\n",
    "# Get best k value\n",
    "best_k, best_accuracy, best_history_df, best_ratings = find_best_k(data, k_values)\n",
    "\n",
    "if best_k is not None and best_history_df is not None:\n",
    "    # Create the directory\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    \n",
    "    # Save the processed history with the best k value\n",
    "    save_history(best_history_df, output_csv)\n",
    "    print(f\"\\nResults saved to: {output_csv}\")\n",
    "\n",
    "    # Print history\n",
    "    print(\"\\nSample of processed fight history with the best k value:\")\n",
    "    print(best_history_df.head().to_string())\n",
    "\n",
    "    # Display top fighters based on elo and save to CSV\n",
    "    display_top_fighters(best_history_df, best_ratings, top_n=50, save_csv=True, csv_path=rankings_csv)\n",
    "    print(f\"\\nTop fighters rankings saved to: {rankings_csv}\")\n",
    "else:\n",
    "    print(\"\\nNo suitable k value found to save fight history.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
